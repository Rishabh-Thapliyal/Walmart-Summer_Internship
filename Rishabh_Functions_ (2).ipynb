{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. Function for initial EDA (initial_eda(df))\n",
    "2. Function to add target column (add_target_column(df))\n",
    "3. Function for train/test split (train_test_split(X_train))\n",
    "4. Function to remove stopwords (remove_stopwords(X_train))\n",
    "5. Function for lemmatization (perform_lemmatization(X_train))\n",
    "6. Function to remove single character words (remove_single_char_words(X_train))\n",
    "7. Function for RF model (Random_Forest_CV(X_train, Y_train))\n",
    "8. Function for LR model (Logistic_Regression_CV(X_train,Y_train))\n",
    "\n",
    "9. Function to add fuzzy score\n",
    "10. Function for LR predict\n",
    "11. Function for RF predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= pd.read_csv('/data/DS_INTERN/data/RAW_DATA/train_test_data.csv', low_memory=False)\n",
    "\n",
    "# df1 = add_target_column(df)\n",
    "\n",
    "# train_df, test_df = train_test_split(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_eda(df):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        total_na = df.isna().sum().sum()\n",
    "        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n",
    "        print(\"Total NA Values : %d \" % (total_na))\n",
    "        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n",
    "        col_name = df.columns\n",
    "        dtyp = df.dtypes\n",
    "        uniq = df.nunique()\n",
    "        na_val = df.isna().sum()\n",
    "        for i in range(len(df.columns)):\n",
    "            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n",
    "        \n",
    "    else:\n",
    "        print(\"Expect a DataFrame but got a %15s\" % (type(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target_column(df):\n",
    "    m1 = df['REVISED_CATEGORY'].isna()\n",
    "    m2 = df['REVISED_VARIETY'].isna()\n",
    "\n",
    "    df['TARGET'] = np.select([m1, m2, m1 & m2], \n",
    "                            [df['REVISED_CATEGORY'], df['REVISED_VARIETY'], np.nan], \n",
    "                            default=df['REVISED_CATEGORY'] + '_' + df['REVISED_VARIETY'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    train_df = df.dropna(subset=['TARGET'])\n",
    "    test_df = df[df['TARGET'].isna()]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(df):\n",
    "    train_df = df.dropna(subset=['TARGET'])\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(df):\n",
    "    test_df = df[df['TARGET'].isna()]\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_preprocessing(df):\n",
    "    \n",
    "    df['FEAT1'] = pd.Series(remove_special_characters(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(perform_spell_correction_manual(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(perform_spell_correction_walmart(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(remove_stopwords(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(remove_numbers(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(perform_lemmatization(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(remove_two_and_single_char_words(df['FEAT1']), index=df.index)\n",
    "    df['FEAT1'] = pd.Series(remove_noise_words(df['FEAT1']), index=df.index)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_preprocessing_series(s):\n",
    "    \n",
    "    s = remove_special_characters(s)\n",
    "    s = perform_spell_correction_manual(s)\n",
    "    s = perform_spell_correction_walmart(s)\n",
    "    s = remove_stopwords(s)\n",
    "    s = remove_numbers(s)\n",
    "    s = perform_lemmatization(s)\n",
    "    s = remove_two_and_single_char_words(s)\n",
    "    s = remove_noise_words(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    s = s.apply(lambda x: re.sub(r\"[^a-zA-Z0-9]+\", ' ',x))\n",
    "                \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(s):\n",
    "    \n",
    "    '''Remove the stopwords from the input series and return the modified series'''\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    #from nltk import download\n",
    "    #download('stopwords')  # Download stopwords list.\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x.lower().split() if word not in (stop_words)]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(s):\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    pattern = '[0-9]'\n",
    "    s = s.apply(lambda x: re.sub(pattern, \"\", x)) \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise_words(s):\n",
    "    \n",
    "    noise_list = ['UNASSIGNED', 'DOTCOM ONLY', 'DOTCOM', 'DOTS COM', 'DSV',\n",
    "                  'ONLINE ONLY', 'ONLINE',  'DO NOT USE', 'STATE FEES',\n",
    "                                        'STATE FEE', 'REDUCED PROGRAM','REDUCED',\n",
    "                                        'ITEM', 'NON TAXABLE', 'TAXABLE',\n",
    "                                        'DELETE', 'PROMO', 'EMPTY FINELINE',\n",
    "                                        'EMPTY FINELINES', 'TEMP', 'XTEMP',\n",
    "                                        'CVP', 'CUSTOMER VALUE PROGRAM',\n",
    "                                        'FINELINE', 'UNKNOWN', 'DESCRIPTION',\n",
    "                                        'WALMART', 'NULL', 'NA', 'DOT COM',\n",
    "                                        'PR', 'HI', 'AK', 'OPEN', 'BLANK',\n",
    "                                        'PUERTO', 'RICO', 'HAWAII', 'ALASKA',\n",
    "                                        'AND', 'THE', 'IC', 'MERCHANDISE',\n",
    "                                        'TO BE DELETED1', 'TO BE DELETED', 'DELETED',\n",
    "                                        'DISCOUNT', 'COUPON', 'FULL WM', 'REVENUE',\n",
    "                                        'DEPT', 'UNTRANSLATABLE','ONLY',\n",
    "                                        '1ST','2ND','MD','MARKDOWN','UNBRANDED']\n",
    "    \n",
    "    s = s.apply(lambda x: ' '.join([word for word in x.split() if word not in (noise_list)]))\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_spell_correction_manual(s):\n",
    "    \n",
    "    dictionary = {'ALMD':'ALMOND','ARTICHOKES':'ARTICHOKE','BACN':'BACON','BLEU':'BLUE',\n",
    "                 'CARTN':'CARTON','CCANDY':'CANDY','CARNS':'CARN',\n",
    "                 'CHICKN':'CHICKEN','CHILIES':'CHILI','CHNKS':'CHUNK','CHNKY':'CHUNKY','CHOC':'CHOCOLATE','CHOPPD':'CHOPPED',\n",
    "                 'CLEMS':'CLEM','COBB':'COB','COKCATAIL':'COCKTAIL','CONCORD':'CONCORDE','CORROGATED':'CORRUGATED',\n",
    "                 'CRSP':'CRISP','ENG':'ENGLISH','ELEVTE':'ELEVATE','FLVRD':'FLAVORED',\n",
    "                 'FLAVRD':'FLAVORED','JUICI':'JUICE','JUICING':'JUICE','MINNEOLAS':'MINNEOLA','PINKLDY':'PINKLADY',\n",
    "                 'SUNFLWER':'SUNFLOWER','SUPERSEEDZ':'SUPERSEEDS','TOVS':'TOV','TROPICALS':'TROPICAL',\n",
    "                 }\n",
    "\n",
    "    \n",
    "    s = s.apply(lambda x: ' '.join([dictionary.get(word,word) for word in x.split()]))\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_spell_correction_walmart(s):\n",
    "    \n",
    "    spell = pd.read_csv('all_corrections_new.csv', low_memory=False)\n",
    "    \n",
    "    spell = spell[spell['SBU_SP']=='FOOD_PRODUCE & FLOWERS'][['key','value']]\n",
    "    dictionary = dict(zip(spell.key, spell.value))\n",
    "    \n",
    "    s = s.apply(lambda x: ' '.join([dictionary.get(word,word) for word in x.split()]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char_words(s):\n",
    "    \n",
    "    '''Remove the single character words like C,N,K,etc from the input series.\n",
    "       It will not remove the single digit numbers like 2,3,4,etc'''\n",
    "    import re \n",
    "    \n",
    "    s = s.apply(lambda x: re.sub(r\"\\b[a-zA-Z]\\b\", \"\", x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_two_and_single_char_words(s):\n",
    "    \n",
    "    import re \n",
    "    \n",
    "    s = s.apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_greater_than_3(word):\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    if len(word)>3:\n",
    "        return lmtzr.lemmatize(word)\n",
    "    else :\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lemmatization(s):\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    s = s.apply(lambda x: [word.lower() for word in x.split()])\n",
    "\n",
    "    # LEMMATIZATION\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst:[lem_greater_than_3(word) for word in lst])\n",
    "    \n",
    "    # Join\n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lemmatization_all(s):\n",
    "    \n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    s = s.apply(lambda x: [word.lower() for word in x.split()])\n",
    "\n",
    "    # LEMMATIZATION\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
    "    \n",
    "    # Join\n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_forest_CV(X_train, Y_train):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import time\n",
    "    \n",
    "    # metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer()),\n",
    "                    ('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "    \n",
    "    scores = cross_validate(pipe, X_train, Y_train, scoring= ('accuracy','precision_micro','recall_micro'), cv=5)\n",
    "\n",
    "    print('Random Forest Classifier performance on cross validation')\n",
    "    print(\"Accuracy : {:0.5f}\".format(scores['test_accuracy'].mean()))\n",
    "    print(\"Precision_micro : {:0.5f}\".format(scores['test_precision_micro'].mean()))\n",
    "    print(\"Recall_micro : {:0.5f}\".format(scores['test_recall_micro'].mean()))\n",
    "    print('time', time.time() - start, '\\n\\n')\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_CV(X_train, Y_train):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import time\n",
    "    \n",
    "    # metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer()),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced'))])\n",
    "    \n",
    "    scores = cross_validate(pipe, X_train, Y_train, scoring= ('accuracy','precision_micro','recall_micro'), cv=5)\n",
    "\n",
    "    print('Logistic Regression Classifier performance on cross validation')\n",
    "    print(\"Accuracy : {:0.5f}\".format(scores['test_accuracy'].mean()))\n",
    "    print(\"Precision_micro : {:0.5f}\".format(scores['test_precision_micro'].mean()))\n",
    "    print(\"Recall_micro : {:0.5f}\".format(scores['test_recall_micro'].mean()))\n",
    "    print('time', time.time() - start, '\\n\\n')\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_LR():\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer()),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced'))])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_RF():\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import time\n",
    "\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer()),\n",
    "                    ('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "     \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_LR_HP(max_iter):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                    ('clf', LogisticRegression(C=250, penalty='l1',solver='saga',class_weight='balanced',max_iter= max_iter))])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_RF_HP():\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import time\n",
    "\n",
    "\n",
    "    pipe = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=220,class_weight='balanced'))])\n",
    "     \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_LR(pipe, train_df):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    df1 = train_df['TARGET'].value_counts().rename_axis('classes').reset_index(name = 'No. of data')\n",
    "    \n",
    "    tt = pd.DataFrame(pipe['clf'].classes_, columns=['classes'])\n",
    "    names = []\n",
    "    coeff = []\n",
    "    for i in range(533):\n",
    "        m = max(abs(pipe['clf'].coef_[i].max()), abs(pipe['clf'].coef_[i].min()))\n",
    "    \n",
    "        coeff.append(pipe['clf'].coef_[i][np.where(abs(pipe['clf'].coef_[i]) == m)[0][0]])\n",
    "        names.append(pipe['vect'].get_feature_names()[np.where(abs(pipe['clf'].coef_[i]) == m)[0][0]])\n",
    "        \n",
    "    tt['most_imp_feat'] = names\n",
    "    tt['coeff'] = coeff\n",
    "    \n",
    "    tt = pd.merge(tt, df1, how='left', on=['classes'])\n",
    "    \n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_voting_hard(solver, penalty, random_state, max_iter):\n",
    "    \n",
    "    from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    # Build the individual models\n",
    "    \n",
    "    pipeline_RF = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                             ('clf', RandomForestClassifier(n_estimators= 220,class_weight='balanced', \n",
    "                                                            random_state=random_state))])\n",
    "\n",
    "    pipeline_LR = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                        ('clf', LogisticRegression(C=250, penalty=penalty, solver=solver,class_weight='balanced',\n",
    "                                                   random_state=random_state, max_iter= max_iter))])\n",
    "\n",
    "    \n",
    "    # List of (string, estimator) tuples\n",
    "    estimators = [('RF', pipeline_RF), ('LR', pipeline_LR)]\n",
    "\n",
    "    # Build and fit an averaging classifier\n",
    "    pipe = VotingClassifier(estimators = estimators)\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_voting_hard_3(solver, random_state, max_iter,kernel):\n",
    "    \n",
    "    from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    # Build the individual models\n",
    "    \n",
    "    pipeline_RF = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                             ('clf', RandomForestClassifier(n_estimators= 220,class_weight='balanced',\n",
    "                                                           random_state=random_state))])\n",
    "\n",
    "    pipeline_LR = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                        ('clf', LogisticRegression(C=250, penalty='l1',solver=solver,class_weight='balanced',\n",
    "                                                   max_iter= max_iter,random_state=random_state))])\n",
    "    \n",
    "    pipeline_SVM = Pipeline([('vect', TfidfVectorizer(min_df=10)),\n",
    "                            ('clf', SVC(class_weight='balanced', kernel=kernel, random_state=random_state))])\n",
    "    \n",
    "    # List of (string, estimator) tuples\n",
    "    estimators = [('RF', pipeline_RF), ('LR', pipeline_LR), ('SVM', pipeline_SVM)]\n",
    "\n",
    "    # Build and fit an averaging classifier\n",
    "    pipe = VotingClassifier(estimators = estimators)\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_voting_soft(max_iter):\n",
    "    \n",
    "    from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    # Build the individual models\n",
    "    pipeline_LR = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                        ('clf', LogisticRegression(C=250, penalty='l1',solver='saga',class_weight='balanced',max_iter= max_iter))])\n",
    "\n",
    "    pipeline_RF = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                             ('clf', RandomForestClassifier(n_estimators= 220,class_weight='balanced'))])\n",
    "\n",
    "    # List of (string, estimator) tuples\n",
    "    estimators = [('RF', pipeline_RF), ('LR', pipeline_LR)]\n",
    "\n",
    "    # Build and fit an averaging classifier\n",
    "    pipe = VotingClassifier(estimators = estimators, voting='soft')\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_voting_soft_3(max_iter, kernel):\n",
    "    \n",
    "    from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    # Build the individual models\n",
    "    pipeline_LR = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                        ('clf', LogisticRegression(C=250, penalty='l1',solver='saga',class_weight='balanced',max_iter= max_iter))])\n",
    "\n",
    "    pipeline_RF = Pipeline([('vect',TfidfVectorizer(min_df=10)),\n",
    "                             ('clf', RandomForestClassifier(n_estimators= 220,class_weight='balanced'))])\n",
    "    \n",
    "    pipeline_SVM = Pipeline([('vect', TfidfVectorizer(min_df=10)),\n",
    "                            ('clf', SVC(class_weight='balanced', kernel=kernel))])\n",
    "\n",
    "    # List of (string, estimator) tuples\n",
    "    estimators = [('RF', pipeline_RF), ('LR', pipeline_LR), ('SVM', pipeline_SVM)]\n",
    "\n",
    "    # Build and fit an averaging classifier\n",
    "    pipe = VotingClassifier(estimators = estimators, voting='soft')\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_stemming(s):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    s = s.apply(lambda x: [word.lower() for word in x.split()])\n",
    "\n",
    "    #the stemmer requires a language parameter\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    s = s.apply(lambda lst:[snow_stemmer.stem(word) for word in lst])\n",
    "    \n",
    "    # Join\n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_spellcheck(s):\n",
    "    \n",
    "    from spellchecker import SpellChecker\n",
    "\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    s = s.apply(lambda x: [word.lower() for word in x.split()])\n",
    "\n",
    "    # SPELLCHECKER\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    s = s.apply(lambda lst:[spell.correction(word) for word in lst])\n",
    "    \n",
    "    # Join\n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_textblob(s):\n",
    "    \n",
    "    from textblob import Word\n",
    "\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    s = s.apply(lambda x: [word.lower() for word in x.split()])\n",
    "\n",
    "    \n",
    "    s = s.apply(lambda lst:[Word(word).spellcheck()[0][0] for word in lst])\n",
    "    \n",
    "    # Join\n",
    "    s = s.apply(lambda x: ' '.join([word.upper() for word in x]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_factor(test_df):\n",
    "    \n",
    "    import Levenshtein as lev\n",
    "    from fuzzywuzzy import fuzz\n",
    "    \n",
    "#    test_df['Predicted_target'] = pd.Series(Y_pred, index= test_df.index)\n",
    "    test_df[['category','variety']] = test_df['Predicted_target'].str.split('_', expand=True)\n",
    "#    test_df.drop(['Predicted_target'], axis=  1, inplace = True)\n",
    "    \n",
    "    test_df['category'] = pd.Series(perform_spell_correction_manual(test_df['category']), index=test_df.index)\n",
    "    test_df['variety'] = pd.Series(perform_spell_correction_manual(test_df['variety']), index=test_df.index)\n",
    "    \n",
    "    test_df['category'] = pd.Series(perform_spell_correction_walmart(test_df['category']), index=test_df.index)\n",
    "    test_df['variety'] = pd.Series(perform_spell_correction_walmart(test_df['variety']), index=test_df.index)\n",
    "    \n",
    "    \n",
    "    test_df['category'] = pd.Series(perform_lemmatization(test_df['category']), index=test_df.index)\n",
    "    test_df['variety'] = pd.Series(perform_lemmatization(test_df['variety']), index=test_df.index)\n",
    "    \n",
    "    \n",
    "    test_df['fuzzy_category'] = test_df.apply(lambda x: fuzz.partial_token_set_ratio(x['FEAT1'],x['category']), axis=1)\n",
    "    test_df['fuzzy_variety'] = test_df.apply(lambda x: fuzz.partial_token_set_ratio(x['FEAT1'],x['variety']), axis=1)\n",
    "    \n",
    "    test_df['TF_cat'] = test_df.apply(lambda x: 1 if x['fuzzy_category']>=80 else 0, axis=1)\n",
    "    test_df['TF_var'] = test_df.apply(lambda x: 1 if x['fuzzy_variety']>=80 else 0, axis=1)\n",
    "    \n",
    "    test_df['TF'] = np.where((test_df['TF_cat'] == 1) | (test_df['TF_var'] ==1),1, 0)\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_simple_wordnet(train_df, n_per_original):\n",
    "    \n",
    "    import random\n",
    "    import snorkel\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "\n",
    "    from snorkel.augmentation import transformation_function\n",
    "\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "\n",
    "    def get_synonyms(word):\n",
    "        \"\"\"Get the synonyms of word from Wordnet.\"\"\"\n",
    "        lemmas = set().union(*[s.lemmas() for s in wn.synsets(word)])\n",
    "        return list(set(l.name().lower().replace(\"_\", \" \") for l in lemmas) - {word})\n",
    "\n",
    "\n",
    "    @transformation_function()\n",
    "    def tf_replace_word_with_synonym(x):\n",
    "        \"\"\"Try to replace a random word with a synonym.\"\"\"\n",
    "        words = [w.lower() for w in x['FEAT1'].split()]    \n",
    "        idx = random.choice(range(len(words)))\n",
    "        synonyms = get_synonyms(words[idx])\n",
    "        if len(synonyms) > 0:\n",
    "            x['FEAT1'] = \" \".join(words[:idx] + [synonyms[0]] + words[idx + 1 :])\n",
    "\n",
    "            x['FEAT1'] = ' '.join([w.upper() for w in x['FEAT1'].split()])\n",
    "\n",
    "            return x\n",
    "        \n",
    "    from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier\n",
    "\n",
    "\n",
    "    tf_policy = ApplyOnePolicy(n_per_original= n_per_original, keep_original=True)\n",
    "    tf_applier = PandasTFApplier([tf_replace_word_with_synonym], tf_policy)\n",
    "\n",
    "\n",
    "    #tt = pd.DataFrame(train_df['FEAT1'])\n",
    "    train_df_augmented = tf_applier.apply(train_df)\n",
    "    \n",
    "    return train_df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_simple_walmart(train_df, n_per_original):\n",
    "    \n",
    "    import random\n",
    "    import re\n",
    "\n",
    "    from snorkel.augmentation import transformation_function\n",
    "    \n",
    "    # csv file from Walmart\n",
    "    syn_df = pd.read_csv('synonyms_v1.csv', sep=',')\n",
    "\n",
    "\n",
    "    def get_synonyms(word):\n",
    "        \"\"\"Get the synonyms of word from Walmart.\"\"\"\n",
    "        l = ((syn_df[syn_df['WORD']== word]['SYNONYMS']))\n",
    "\n",
    "        if l.shape[0] == 0:\n",
    "            return []\n",
    "        else :\n",
    "\n",
    "#             l= l.iloc[0]\n",
    "#             # to convert string to list\n",
    "#             Syn_List = re.sub(\"[^\\w]\", \" \",  l).split()\n",
    "            Syn_List = l.iloc[0].split(',')\n",
    "\n",
    "            if len(Syn_List):\n",
    "                idx = random.choice(range(len(Syn_List)))\n",
    "                return (Syn_List[idx])\n",
    "\n",
    "\n",
    "    @transformation_function()\n",
    "    def tf_replace_word_with_synonym(x):\n",
    "        \"\"\"Try to replace a random word with a synonym.\"\"\"\n",
    "        words = [w for w in x['FEAT1'].split()]    \n",
    "        idx = random.choice(range(len(words)))\n",
    "        synonyms = get_synonyms(words[idx])\n",
    "        if len(synonyms) > 0:\n",
    "            x['FEAT1'] = \" \".join(words[:idx] + [synonyms] + words[idx + 1 :])\n",
    "            return x\n",
    "        \n",
    "    from snorkel.augmentation import ApplyOnePolicy, PandasTFApplier\n",
    "\n",
    "    tf_policy = ApplyOnePolicy(n_per_original=n_per_original, keep_original=True)\n",
    "    tf_applier = PandasTFApplier([tf_replace_word_with_synonym], tf_policy)\n",
    "    train_df_augmented = tf_applier.apply(train_df)\n",
    "    \n",
    "    return train_df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_complex_walmart(train_df, n_per_original,sequence_length, verb, noun, adjective):\n",
    "    \n",
    "    import nltk\n",
    "    import re\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    nltk.download(\"wordnet\")\n",
    "    from snorkel.augmentation import transformation_function\n",
    "    from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "    spacy = SpacyPreprocessor(text_field=\"FEAT1\", doc_field=\"doc\", memoize=False)\n",
    "\n",
    "    # csv from Walmart\n",
    "    syn_df = pd.read_csv('synonyms_v1.csv', sep=',')\n",
    "\n",
    "\n",
    "    def get_synonym(word, pos=None):\n",
    "        \"\"\"Get the synonyms of word from Walmart.\"\"\"\n",
    "\n",
    "        l = ((syn_df[syn_df['WORD']== word]['SYNONYMS']))\n",
    "\n",
    "        if l.shape[0] == 0:\n",
    "            return []\n",
    "        else :\n",
    "\n",
    "#             l= l.iloc[0]\n",
    "\n",
    "#             Syn_List = re.sub(\"[^\\w]\", \" \",  l).split()\n",
    "            Syn_List = l.iloc[0].split(',')\n",
    "\n",
    "            if len(Syn_List):\n",
    "                idx = random.choice(range(len(Syn_List)))\n",
    "                return (Syn_List[idx])\n",
    "\n",
    "\n",
    "    def replace_token(spacy_doc, idx, replacement):\n",
    "        \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "        p= \" \".join([spacy_doc[:idx].text, replacement,spacy_doc[1 + idx :].text])\n",
    "        return p\n",
    "\n",
    "\n",
    "    @transformation_function(pre=[spacy])\n",
    "    def replace_verb_with_synonym(x):\n",
    "        # Get indices of verb tokens in sentence.\n",
    "\n",
    "        verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "\n",
    "        if verb_idxs:\n",
    "            # Pick random verb idx to replace.\n",
    "            idx = np.random.choice(verb_idxs)\n",
    "            synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "\n",
    "            # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "            if synonym:\n",
    "\n",
    "                x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "                return x\n",
    "\n",
    "\n",
    "    @transformation_function(pre=[spacy])\n",
    "    def replace_noun_with_synonym(x):\n",
    "\n",
    "        # Get indices of noun tokens in sentence.\n",
    "        noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "        if noun_idxs:\n",
    "            \n",
    "            # Pick random noun idx to replace.\n",
    "            idx = np.random.choice(noun_idxs)\n",
    "            synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "\n",
    "            # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "            if synonym:\n",
    "                x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "\n",
    "                return x\n",
    "\n",
    "\n",
    "    @transformation_function(pre=[spacy])\n",
    "    def replace_adjective_with_synonym(x):\n",
    "        # Get indices of adjective tokens in sentence.\n",
    "        adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "        if adjective_idxs:\n",
    "            \n",
    "            # Pick random adjective idx to replace.\n",
    "            idx = np.random.choice(adjective_idxs)\n",
    "            synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "    \n",
    "            # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "            if synonym:\n",
    "                x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "                return x\n",
    "           \n",
    "    from snorkel.augmentation import PandasTFApplier, MeanFieldPolicy\n",
    "\n",
    "    tfs = [\n",
    "        replace_verb_with_synonym,\n",
    "        replace_noun_with_synonym,\n",
    "        replace_adjective_with_synonym,\n",
    "    ]\n",
    "\n",
    "    mean_field_policy = MeanFieldPolicy(\n",
    "        len(tfs),\n",
    "        sequence_length=sequence_length,\n",
    "        n_per_original=n_per_original,\n",
    "        keep_original=True,\n",
    "        p=[verb, noun, adjective]\n",
    "    )\n",
    "    \n",
    "    tf_applier = PandasTFApplier(tfs, mean_field_policy)\n",
    "    train_df_augmented = tf_applier.apply(train_df)\n",
    "    \n",
    "    return train_df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item1_desc_mapping(train_df, test_df):\n",
    "    \n",
    "    # first count the unique target on grouping\n",
    "    tt = train_df.groupby(['ITEM1_DESC'], sort=False)['TARGET'].agg([('count_distinct_target', 'nunique')]).reset_index()\n",
    "\n",
    "    # select only the ones with count=1\n",
    "    ttt= tt[(tt['count_distinct_target']==1)]\n",
    "\n",
    "    # list of ITEM1_DESC\n",
    "    train_list_item = []\n",
    "    for i in range(len(ttt)):\n",
    "        train_list_item.append(ttt.iloc[i]['ITEM1_DESC'])\n",
    "\n",
    "    #list of ITEM1_DESC for test\n",
    "    test_list_item = []\n",
    "    for i in range(len(test_df)):\n",
    "        test_list_item.append(test_df.iloc[i]['ITEM1_DESC'])\n",
    "\n",
    "    common_list = list(set(train_list_item).intersection(set(test_list_item)))\n",
    "    \n",
    "    # ITEM1_DESC mapping and updating the predicted_target\n",
    "\n",
    "    for i in range(len(common_list)):\n",
    "        item = common_list[i]\n",
    "        j = test_df.iloc[np.where(test_df['ITEM1_DESC']==item)].index\n",
    "        for k in range(len(j)):\n",
    "            test_df.loc[j[k]]['Predicted_target'] = train_df[train_df['ITEM1_DESC']==item]['TARGET'].iloc[0]\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "# Import module for iteration\n",
    "import itertools\n",
    "# Import module for function development\n",
    "from typing import Union, List, Tuple\n",
    "# Import module for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import module for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import module for KNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# String matching - TF-IDF\n",
    "def build_vectorizer(\n",
    "    clean: pd.Series,\n",
    "    analyzer: str = 'char', \n",
    "    ngram_range: Tuple[int, int] = (1, 4), \n",
    "    n_neighbors: int = 1, \n",
    "    **kwargs\n",
    "    ) -> Tuple:\n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, **kwargs)\n",
    "    X = vectorizer.fit_transform(clean.values.astype('U'))\n",
    "\n",
    "    # Fit nearest neighbors corpus\n",
    "    nbrs = NearestNeighbors(n_neighbors = n_neighbors, metric = 'cosine').fit(X)\n",
    "    return vectorizer, nbrs\n",
    "\n",
    "# String matching - KNN\n",
    "def tfidf_nn(\n",
    "    messy, \n",
    "    clean, \n",
    "    n_neighbors = 1, \n",
    "    **kwargs\n",
    "    ):\n",
    "    # Fit clean data and transform messy data\n",
    "    vectorizer, nbrs = build_vectorizer(clean, n_neighbors = n_neighbors, **kwargs)\n",
    "    input_vec = vectorizer.transform(messy)\n",
    "\n",
    "    # Determine best possible matches\n",
    "    distances, indices = nbrs.kneighbors(input_vec, n_neighbors = n_neighbors)\n",
    "    nearest_values = np.array(clean)[indices]\n",
    "    return nearest_values, distances\n",
    "\n",
    "def fn_get_ratio(ratio_type, partial=False):\n",
    "    \n",
    "    if ratio_type == 'set' and partial == False:\n",
    "        return fuzz.token_set_ratio\n",
    "    elif ratio_type == 'set' and partial == True:\n",
    "        return fuzz.partial_token_set_ratio\n",
    "    elif ratio_type == 'sort' and partial == False:\n",
    "        return fuzz.token_sort_ratio\n",
    "    elif ratio_type == 'sort' and partial == True:\n",
    "        return fuzz.partial_token_sort_ratio\n",
    "    else:\n",
    "        return(\"Please provide a valid combination of ratio_type and partial parameter\")\n",
    "    \n",
    "# String matching - match fuzzy\n",
    "def find_matches_fuzzy(\n",
    "    row, \n",
    "    match_candidates,\n",
    "    partial, \n",
    "    ratio_type,\n",
    "    limit = 5\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    scorer_fn = fn_get_ratio(partial = partial, ratio_type = ratio_type)\n",
    "    row_matches = process.extract(\n",
    "        row, dict(enumerate(match_candidates)), \n",
    "#         scorer = fuzz.token_sort_ratio, \n",
    "        scorer = scorer_fn,\n",
    "        limit = limit\n",
    "        )\n",
    "    result = [(row, match[0], match[1]) for match in row_matches]\n",
    "    return result\n",
    "\n",
    "def fuzzy_nn_match1(\n",
    "    messy,\n",
    "    clean,\n",
    "    column,\n",
    "    col,partial, ratio_type,\n",
    "    n_neighbors = 100,\n",
    "    limit = 1,  **kwargs):\n",
    "    nearest_values, _ = tfidf_nn(messy, clean, n_neighbors, **kwargs)\n",
    "\n",
    "    results = [find_matches_fuzzy(row, nearest_values[i], partial, ratio_type, limit ) for i, row in enumerate(messy)]\n",
    "    \n",
    "    return ((results))\n",
    "\n",
    "def fuzzy_tfidf_feat1(train_df, test_df, partial, ratio_type):\n",
    "    \n",
    "    dff = fuzzy_nn_match1(test_df['FEAT1'], train_df['FEAT1'],'FEAT1', 'Result', partial=partial, ratio_type=ratio_type)\n",
    "    \n",
    "    major_list = []\n",
    "\n",
    "    for row in range(len(dff)):\n",
    "        major_list.append(pd.DataFrame(dff[row], columns = ['FEAT1_test','FEAT1_train', 'Ratio']))\n",
    "        \n",
    "    major_df = pd.concat(major_list)\n",
    "    major_df.reset_index(inplace=True)\n",
    "    major_df.drop(['index'],axis=1, inplace=True)\n",
    "    \n",
    "#     print('No. of observations with Ratio greater than threshold:',len(major_df[major_df['Ratio']>=ratio_threshold]))\n",
    "    \n",
    "    \n",
    "#     FEAT1_TARGET_mapping_train = dict(zip(train_df.FEAT1, train_df.TARGET))\n",
    "    \n",
    "#     for row in range(len(major_df)):\n",
    "#         data_row = major_df.iloc[row]\n",
    "#         if data_row['Ratio'] >=ratio_threshold:\n",
    "#             test_df.iloc[row]['Predicted_target'] = FEAT1_TARGET_mapping_train[data_row['FEAT1_train']]\n",
    "            \n",
    "    return major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
