{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c236574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from e import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "df= pd.read_csv('/data/DS_INTERN/data/RAW_DATA/train_test_data.csv', low_memory=False)\n",
    "\n",
    "df1 = add_target_column(df)\n",
    "\n",
    "train_df , test_df = train_test_split(df1)\n",
    "\n",
    "train_df = perform_preprocessing(train_df)\n",
    "test_df = perform_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a701e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "spacy = SpacyPreprocessor(text_field=\"FEAT1\", doc_field=\"doc\", memoize=False)\n",
    "\n",
    "\n",
    "syn_df = pd.read_csv('synonyms_sample.csv', sep=';',header=None, names=['word','synonyms'])\n",
    "\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get the synonyms of word from Wordnet.\"\"\"\n",
    "    \n",
    "    l = ((syn_df[syn_df['word']== word]['synonyms']))\n",
    "    \n",
    "    if l.shape[0] == 0:\n",
    "        return []\n",
    "    else :\n",
    "        \n",
    "        l= l.iloc[0]\n",
    "        \n",
    "        Syn_List = re.sub(\"[^\\w]\", \" \",  l).split()\n",
    "\n",
    "        if len(Syn_List):\n",
    "            idx = random.choice(range(len(Syn_List)))\n",
    "            return (Syn_List[idx])\n",
    "\n",
    "        \n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    p= \" \".join([spacy_doc[:idx].text, replacement,spacy_doc[1 + idx :].text])\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    \n",
    "    verb_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    \n",
    "    if verb_idxs:\n",
    "        # Pick random verb idx to replace.\n",
    "        idx = np.random.choice(verb_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"v\")\n",
    "#        print(synonym,'verb')\n",
    "        # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "#            print(synonym.upper())\n",
    "            x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    \n",
    "    # Get indices of noun tokens in sentence.\n",
    "    noun_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "    if noun_idxs:\n",
    "        # Pick random noun idx to replace.\n",
    "        idx = np.random.choice(noun_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"n\")\n",
    "#         print(x.doc[idx].text)\n",
    "#        print(synonym, 'noun')\n",
    "        # If there's a valid noun synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "            \n",
    "            return x\n",
    "\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    # Get indices of adjective tokens in sentence.\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adjective_idxs:\n",
    "        # Pick random adjective idx to replace.\n",
    "        idx = np.random.choice(adjective_idxs)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=\"a\")\n",
    "#        print(synonym,' adjective')\n",
    "        # If there's a valid adjective synonym, replace it. Otherwise, return None.\n",
    "        if synonym:\n",
    "            x.FEAT1 = replace_token(x.doc, idx, synonym.upper())\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c95163ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# syn = wn.synsets('beverage')\n",
    "# words = [lemma.name() for lemma in syn[0].lemmas()]\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "911c00ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48197/48197 [22:11<00:00, 36.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import PandasTFApplier, MeanFieldPolicy\n",
    "\n",
    "tfs = [\n",
    "    replace_verb_with_synonym,\n",
    "    replace_noun_with_synonym,\n",
    "    replace_adjective_with_synonym,\n",
    "]\n",
    "\n",
    "mean_field_policy = MeanFieldPolicy(\n",
    "    len(tfs),\n",
    "    sequence_length=2,\n",
    "    n_per_original=2,\n",
    "    keep_original=True,\n",
    "    p=[0, 0, 1],\n",
    ")\n",
    "\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, mean_field_policy)\n",
    "\n",
    "train_df_augmented = tf_applier.apply(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92744343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ce2a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48197, 48197)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_augmented), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a27d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e75e457e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT1</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BHF VANILLA CHAI LATTE VANILLA CHAI VANILLA CH...</td>\n",
       "      <td>BEVERAGES_BOLTHOUSE JUICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENERGY ACAI BERRY JUICE SAM ENERGY JUICE PREMI...</td>\n",
       "      <td>BEVERAGES_BOLTHOUSE JUICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GENESIS TODAY POM BERRY POM BERRY GENESIS TODA...</td>\n",
       "      <td>BEVERAGES_BOLTHOUSE JUICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PEPPERMINT MOCHA BHF PEPMINT MOCHA PEPPERMINT ...</td>\n",
       "      <td>BEVERAGES_BOLTHOUSE JUICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SAMBAZON ACAI BERRY PACK SAM CAL ACAI JUICE PR...</td>\n",
       "      <td>BEVERAGES_BOLTHOUSE JUICE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                FEAT1  \\\n",
       "8   BHF VANILLA CHAI LATTE VANILLA CHAI VANILLA CH...   \n",
       "9   ENERGY ACAI BERRY JUICE SAM ENERGY JUICE PREMI...   \n",
       "10  GENESIS TODAY POM BERRY POM BERRY GENESIS TODA...   \n",
       "11  PEPPERMINT MOCHA BHF PEPMINT MOCHA PEPPERMINT ...   \n",
       "12  SAMBAZON ACAI BERRY PACK SAM CAL ACAI JUICE PR...   \n",
       "\n",
       "                       TARGET  \n",
       "8   BEVERAGES_BOLTHOUSE JUICE  \n",
       "9   BEVERAGES_BOLTHOUSE JUICE  \n",
       "10  BEVERAGES_BOLTHOUSE JUICE  \n",
       "11  BEVERAGES_BOLTHOUSE JUICE  \n",
       "12  BEVERAGES_BOLTHOUSE JUICE  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ace2766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_augmented['FEAT1']\n",
    "Y_train = train_df_augmented['TARGET']\n",
    "\n",
    "X_test = test_df['FEAT1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5be3d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline_voting_hard(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67ad2d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 482.179808139801 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/libraries/nb_env/lib64/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "print('time', time.time() - start, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "607b12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c566fe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fuzzy_category</th>\n",
       "      <th>fuzzy_variety</th>\n",
       "      <th>TF_cat</th>\n",
       "      <th>TF_var</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>248208.000000</td>\n",
       "      <td>248208.000000</td>\n",
       "      <td>248208.000000</td>\n",
       "      <td>248208.000000</td>\n",
       "      <td>248208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>82.714220</td>\n",
       "      <td>88.905575</td>\n",
       "      <td>0.705622</td>\n",
       "      <td>0.799809</td>\n",
       "      <td>0.915204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27.466785</td>\n",
       "      <td>22.960907</td>\n",
       "      <td>0.455764</td>\n",
       "      <td>0.400144</td>\n",
       "      <td>0.278578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fuzzy_category  fuzzy_variety         TF_cat         TF_var  \\\n",
       "count   248208.000000  248208.000000  248208.000000  248208.000000   \n",
       "mean        82.714220      88.905575       0.705622       0.799809   \n",
       "std         27.466785      22.960907       0.455764       0.400144   \n",
       "min          0.000000       0.000000       0.000000       0.000000   \n",
       "25%         50.000000     100.000000       0.000000       1.000000   \n",
       "50%        100.000000     100.000000       1.000000       1.000000   \n",
       "75%        100.000000     100.000000       1.000000       1.000000   \n",
       "max        100.000000     100.000000       1.000000       1.000000   \n",
       "\n",
       "                  TF  \n",
       "count  248208.000000  \n",
       "mean        0.915204  \n",
       "std         0.278578  \n",
       "min         0.000000  \n",
       "25%         1.000000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = trust_factor(Y_pred, test_df)\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da64230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7e3552f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline_voting_hard(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87db720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/libraries/nb_env/lib64/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/home/jupyter/libraries/nb_env/lib64/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/jupyter/libraries/nb_env/lib64/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/jupyter/libraries/nb_env/lib64/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "scores = cross_validate(pipe, X_train, Y_train, scoring= ('accuracy','precision_micro','recall_micro'), cv=5)\n",
    "\n",
    "print(\"Accuracy : {:0.5f}\".format(scores['test_accuracy'].mean()))\n",
    "print(\"Precision_micro : {:0.5f}\".format(scores['test_precision_micro'].mean()))\n",
    "print(\"Recall_micro : {:0.5f}\".format(scores['test_recall_micro'].mean()))\n",
    "\n",
    "\n",
    "print('time', time.time() - start, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e52af",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb5af16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3032d292ba87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd4941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97e731b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-766a4b277272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load BERT and the preprocessing model from TF Hub.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# Load BERT and the preprocessing model from TF Hub.\n",
    "preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "encoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6339ab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-94d9f6893771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = keras.Sequential([\n\u001b[1;32m      5\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75665ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f8c7c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a95cdad765f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'keras'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82c597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61256d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
